{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fe3fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\priyanshi\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "        num_pages = pdf_reader.numPages\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            text += page.extractText()\n",
    "\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"sample.pdf\"\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16bcdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.22.5-cp39-cp39-win_amd64.whl (11.8 MB)\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.22.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"sample.pdf\"\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "        num_pages = pdf_reader.numPages\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            text += page.extractText()\n",
    "\n",
    "            # Process text from the current page (optional)\n",
    "            # process_text(text)\n",
    "\n",
    "            # Reset text to save memory (optional)\n",
    "            text = \"\"\n",
    "\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"large_sample.pdf\"\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a864a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf_document:\n",
    "        num_pages = len(pdf_document.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_document.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"large_sample.pdf\"\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4512948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_technical_experience(text):\n",
    "    pattern = r\"TECHNICAL EXPERIENCE\\nâ€¢ (.*?)\\n(.*?)\\n(.*?)\\n\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    experience_list = []\n",
    "\n",
    "    for match in matches:\n",
    "        experience = {\n",
    "            \"key\": match[0],\n",
    "            \"start_date\": match[1].strip(),\n",
    "            \"description\": match[2].strip(),\n",
    "        }\n",
    "        experience_list.append(experience)\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"your_large_pdf.pdf\"\n",
    "    with open(pdf_path, \"r\") as file:\n",
    "        pdf_text = file.read()\n",
    "\n",
    "    technical_experience_list = parse_technical_experience(pdf_text)\n",
    "    print(technical_experience_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b16ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_technical_experience(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    experience_list = []\n",
    "    current_experience = {}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == \"technical\" and token.nbor().text.lower() == \"experience\":\n",
    "            if current_experience:\n",
    "                experience_list.append(current_experience)\n",
    "                current_experience = {}\n",
    "            continue\n",
    "\n",
    "        if not current_experience.get(\"key\"):\n",
    "            current_experience[\"key\"] = token.text\n",
    "        else:\n",
    "            if not current_experience.get(\"description\"):\n",
    "                current_experience[\"description\"] = token.text\n",
    "            else:\n",
    "                current_experience[\"description\"] += \" \" + token.text\n",
    "\n",
    "    if current_experience:\n",
    "        experience_list.append(current_experience)\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"your_large_pdf.pdf\"\n",
    "    with open(pdf_path, \"r\") as file:\n",
    "        pdf_text = file.read()\n",
    "\n",
    "    technical_experience_list = extract_technical_experience(pdf_text)\n",
    "    print(technical_experience_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1743e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_technical_experience(text):\n",
    "    pattern = r\"(\\d+\\.)\\s(.*?\\d+\\.\\d+.*?)\\n(.*?)\\n\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    experience_list = []\n",
    "\n",
    "    for match in matches:\n",
    "        experience = {\n",
    "            \"title\": match[0].strip(),\n",
    "            \"key\": match[1].strip(),\n",
    "            \"description\": match[2].strip(),\n",
    "        }\n",
    "        experience_list.append(experience)\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"your_large_pdf.pdf\"\n",
    "    with open(pdf_path, \"r\") as file:\n",
    "        pdf_text = file.read()\n",
    "\n",
    "    technical_experience_list = parse_technical_experience(pdf_text)\n",
    "    print(technical_experience_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, start_page=0):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "        for page_num in range(start_page, num_pages):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "def parse_technical_experience(text):\n",
    "    pattern = r\"(\\d+\\.)\\s(.*?\\d+\\.\\d+.*?)\\n(.*?)\\n\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    experience_list = []\n",
    "\n",
    "    for match in matches:\n",
    "        experience = {\n",
    "            \"title\": match[0].strip(),\n",
    "            \"key\": match[1].strip(),\n",
    "            \"description\": match[2].strip(),\n",
    "        }\n",
    "        experience_list.append(experience)\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"your_large_pdf.pdf\"\n",
    "\n",
    "    # Set the starting page number to exclude the index pages (e.g., 3)\n",
    "    starting_page = 3\n",
    "\n",
    "    pdf_text = extract_text_from_pdf(pdf_path, starting_page)\n",
    "    technical_experience_list = parse_technical_experience(pdf_text)\n",
    "    print(technical_experience_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'name': [\n",
    "    'priyanhi-shukla-good girlnope joker',\n",
    "    'potato beans goodaamir is good',\n",
    "    'shubhandi-makes good food(C1-02-10)-nice gulab jal',\n",
    "    'my name khan(C2-10-67)- Shahrukh khan Kajol',\n",
    "    'kisi ka bhayi kisi ki jan(C2-10-67)- Soleman bhayi Kajol'\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample extracted text\n",
    "extracted_text = \"\"\"\n",
    "India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th. \n",
    "Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively.\n",
    "\n",
    "Long and Short Paragraphs on Pollution in English\n",
    "Below we have provided both long and short paragraphs on pollution of varying word lengths...\n",
    "\"\"\"\n",
    "\n",
    "# Function to calculate match scores\n",
    "def calculate_match_scores(target, names):\n",
    "    match_scores = process.extract(target, names, scorer=fuzz.token_set_ratio)\n",
    "    return match_scores\n",
    "\n",
    "# Extract names from the dataframe\n",
    "names_to_match = df['name'].tolist()\n",
    "\n",
    "# Calculate match scores for each name\n",
    "match_scores = calculate_match_scores(extracted_text, names_to_match)\n",
    "\n",
    "# Combine match scores with original dataframe\n",
    "df['match_scores'] = match_scores\n",
    "\n",
    "# Print the dataframe with match scores\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1435d",
   "metadata": {},
   "source": [
    "### to find keywords as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7dbb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'name': [\n",
    "    'priyanhi-shukla-good girlnope joker',\n",
    "    'potato beans goodaamir is good',\n",
    "    'shubhandi-makes good food(C1-02-10)-nice gulab jal',\n",
    "    'my name khan(C2-10-67)- Shahrukh khan Kajol',\n",
    "    'kisi ka bhayi kisi ki jan(C2-10-67)- Soleman bhayi Kajol'\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample extracted text\n",
    "extracted_text = \"\"\"\n",
    "India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th. \n",
    "Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively.\n",
    "\n",
    "Long and Short Paragraphs on Pollution in English\n",
    "Below we have provided both long and short paragraphs on pollution of varying word lengths...\n",
    "\"\"\"\n",
    "\n",
    "# Function to calculate match score and matching keywords\n",
    "def calculate_match_info(target, name):\n",
    "    match_score = fuzz.token_set_ratio(target, name)\n",
    "    matching_keywords = process.extractOne(target, [name])[0]\n",
    "    return match_score, matching_keywords\n",
    "\n",
    "# Calculate match scores and matching keywords for each name\n",
    "df['match_scores'], df['matching_keywords'] = zip(*df['name'].apply(lambda x: calculate_match_info(extracted_text, x)))\n",
    "\n",
    "# Print the dataframe with match scores and matching keywords\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634d747",
   "metadata": {},
   "source": [
    "## To create new column of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c440f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'name': [\n",
    "    'priyanhi-shukla-good girlnope joker',\n",
    "    'potato beans goodaamir is good',\n",
    "    'shubhandi-makes good food(C1-02-10)-nice gulab jal',\n",
    "    'my name khan(C2-10-67)- Shahrukh khan Kajol',\n",
    "    'kisi ka bhayi kisi ki jan(C2-10-67)- Soleman bhayi Kajol'\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample extracted text with paragraphs\n",
    "extracted_text = \"\"\"\n",
    "India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th. \n",
    "Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively.\n",
    "\n",
    "Long and Short Paragraphs on Pollution in English\n",
    "Below we have provided both long and short paragraphs on pollution of varying word lengths...\n",
    "\n",
    "Paragraph on Pollution 1 (100 Words)\n",
    "Environmental pollution refers to the presence of harmful and poisonous substances into our environment...\n",
    "\n",
    "Paragraph on Pollution 2 (150 Words)\n",
    "Pollution has tremendous adverse affects on environment, resulting in its degradation and also in depletion of living species...\n",
    "\"\"\"\n",
    "\n",
    "# Split extracted text into paragraphs\n",
    "paragraphs = extracted_text.split('\\n\\n')\n",
    "\n",
    "# Remove empty paragraphs and join them into a single column\n",
    "pdf_paragraph = '\\n\\n'.join(p.strip() for p in paragraphs if p.strip())\n",
    "\n",
    "# Add the \"PDF_Paragraph\" column to the dataframe\n",
    "df['PDF_Paragraph'] = pdf_paragraph\n",
    "\n",
    "# Print the updated dataframe with the PDF_Paragraph column\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c512c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe\n",
    "data = {'name': [\n",
    "    'priyanhi-shukla-good girlnope joker',\n",
    "    'potato beans goodaamir is good',\n",
    "    'shubhandi-makes good food(C1-02-10)-nice gulab jal',\n",
    "    'my name khan(C2-10-67)- Shahrukh khan Kajol',\n",
    "    'kisi ka bhayi kisi ki jan(C2-10-67)- Soleman bhayi Kajol'\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sample extracted text\n",
    "extracted_text = \"\"\"\n",
    "1. Objective\n",
    "1.11 India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th. \n",
    "Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively\n",
    "1.2 Long and Short Paragraphs on Pollution in English\n",
    "Below we have provided both long and short paragraphs on pollution of varying word lengths...\n",
    "\n",
    "1.3 Environmental pollution refers to the presence of harmful and poisonous substances into our environment...\n",
    "2. Application development authority\n",
    "2.1 Mobile application development is the process of making software for smartphones, tablets and digital assistants...\n",
    "2.3 Mobile app development is rapidly growing. From retail, telecommunications and e-commerce to insurance...\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression pattern to match paragraphs based on numbering\n",
    "paragraph_pattern = r'\\d+\\.\\d+\\s+.*?(?=\\d+\\.\\d+|$)'\n",
    "\n",
    "# Extract paragraphs using regex\n",
    "paragraphs = re.findall(paragraph_pattern, extracted_text, re.DOTALL)\n",
    "\n",
    "# Combine paragraphs into a single \"PDF_Paragraph\" column\n",
    "pdf_paragraph = '\\n'.join(paragraphs)\n",
    "\n",
    "# Add the \"PDF_Paragraph\" column to the dataframe\n",
    "df['PDF_Paragraph'] = pdf_paragraph\n",
    "\n",
    "# Print the updated dataframe with the PDF_Paragraph column\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Sample dataframe with text column\n",
    "data = {'text_column': [\n",
    "    'India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th.',\n",
    "    'Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively.',\n",
    "    'Long and Short Paragraphs on Pollution in English',\n",
    "    'Environmental pollution refers to the presence of harmful and poisonous substances into our environment...',\n",
    "    'Mobile application development is the process of making software for smartphones, tablets and digital assistants...'\n",
    "]}\n",
    "text_df = pd.DataFrame(data)\n",
    "\n",
    "# Sample extracted text\n",
    "extracted_text = \"\"\"\n",
    "1. Objective\n",
    "1.11 India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th. \n",
    "Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively\n",
    "1.2 Long and Short Paragraphs on Pollution in English\n",
    "Below we have provided both long and short paragraphs on pollution of varying word lengths...\n",
    "\n",
    "1.3 Environmental pollution refers to the presence of harmful and poisonous substances into our environment...\n",
    "2. Application development authority\n",
    "2.1 Mobile application development is the process of making software for smartphones, tablets and digital assistants...\n",
    "2.3 Mobile app development is rapidly growing. From retail, telecommunications and e-commerce to insurance...\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression pattern to match paragraphs based on numbering\n",
    "paragraph_pattern = r'\\d+\\.\\d+\\s+.*?(?=\\d+\\.\\d+|$)'\n",
    "\n",
    "# Extract paragraphs using regex\n",
    "paragraphs = re.findall(paragraph_pattern, extracted_text, re.DOTALL)\n",
    "\n",
    "# Create a DataFrame with paragraphs as rows\n",
    "paragraph_df = pd.DataFrame({'Paragraph': paragraphs})\n",
    "\n",
    "# Calculate match scores for each paragraph\n",
    "def calculate_match_scores(target, paragraphs):\n",
    "    match_scores = paragraphs.apply(lambda p: fuzz.token_set_ratio(target, p))\n",
    "    return match_scores\n",
    "\n",
    "# Match each row from the \"text_column\" in text_df with paragraphs in paragraph_df\n",
    "for index, row in text_df.iterrows():\n",
    "    target_text = row['text_column']\n",
    "    paragraph_df[f'Match_Score_{index}'] = calculate_match_scores(target_text, paragraph_df['Paragraph'])\n",
    "\n",
    "# Print the DataFrame with match scores\n",
    "print(paragraph_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89515af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Sample dataframe with text column\n",
    "data = {'text_column': [\n",
    "    'India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th.',\n",
    "    'Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively.',\n",
    "    'Long and Short Paragraphs on Pollution in English',\n",
    "    'Environmental pollution refers to the presence of harmful and poisonous substances into our environment...',\n",
    "    'Mobile application development is the process of making software for smartphones, tablets and digital assistants...'\n",
    "]}\n",
    "text_df = pd.DataFrame(data)\n",
    "\n",
    "# Sample extracted text\n",
    "extracted_text = \"\"\"\n",
    "1. Objective\n",
    "1.11 India is followed by the Democratic Republic of Congo on 178th rank, Bangladesh on 179th and Burundi on 180th. \n",
    "Top five nations with the best EPI index are Switzerland, France, Denmark, Malta and Sweden respectively\n",
    "1.2 Long and Short Paragraphs on Pollution in English\n",
    "Below we have provided both long and short paragraphs on pollution of varying word lengths...\n",
    "\n",
    "1.3 Environmental pollution refers to the presence of harmful and poisonous substances into our environment...\n",
    "2. Application development authority\n",
    "2.1 Mobile application development is the process of making software for smartphones, tablets and digital assistants...\n",
    "2.3 Mobile app development is rapidly growing. From retail, telecommunications and e-commerce to insurance...\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression pattern to match paragraphs based on numbering\n",
    "paragraph_pattern = r'\\d+\\.\\d+\\s+.*?(?=\\d+\\.\\d+|$)'\n",
    "\n",
    "# Extract paragraphs using regex\n",
    "paragraphs = re.findall(paragraph_pattern, extracted_text, re.DOTALL)\n",
    "\n",
    "# Create a DataFrame with paragraphs as rows\n",
    "paragraph_df = pd.DataFrame({'Paragraph': paragraphs})\n",
    "\n",
    "# Create a new DataFrame to store matching results\n",
    "match_results = []\n",
    "\n",
    "# Match each row from the \"text_column\" in text_df with paragraphs in paragraph_df\n",
    "for index, row in text_df.iterrows():\n",
    "    target_text = row['text_column']\n",
    "    match_score, match = paragraph_df['Paragraph'].apply(lambda p: (fuzz.token_set_ratio(target_text, p), p)).max()\n",
    "    match_results.append({'Paragraph': match, 'Match': target_text, 'Match Score': match_score})\n",
    "\n",
    "# Create a DataFrame from match_results\n",
    "match_df = pd.DataFrame(match_results)\n",
    "\n",
    "# Print the resulting DataFrame with \"Paragraph\", \"Match\", and \"Match Score\" columns\n",
    "print(match_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
